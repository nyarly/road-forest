Also: "blob" resource
Not yourself - simple file service
  allowed_methods - GET, HEAD
  post_is_create: default
  process_post: default
  content_types_accepted: defaults
Yourself - file transfer endpoint
  allowed_methods - GET, HEAD, POST, PUT, DELETE
  post_is_create: true
  process_post: default (unused)
  content_types_accepted: @model.update(params, blob) - update vs. create?

Other concerns:
Content type handling (iow: RDF<->text format)
Authentication
Authorization
Content encoding (gzip, compress)
Charsets
Languages
Exception handling

HTML related:
  Method coercion (POST that means DELETE/PUT)
  Params -> graph
  Form rendering

Cacheing - last_modified, expires, etag(+ W/)

Blending concern-focused modules e.g. content_types_accepted - quality metrics,
accept variants...  variance, conflict, options are related to the above (i.e.
blended concerns)

test build_graph

test credence generations

property delete

property replace

repo autovacuum

collect vocabs used to generate a graph - build curies for representations that work that way

BGP queries should be solveable *across* resources - currently all patterns get
a :context variable applied that all have to match, but it seems reasonable
that multiple resources might participate in a solution.


*** API challenges ***

Client behavior should get packed up in a tidy bunch of chains and blocks, but
if there's an error, Ruby's default exceptions aren't helpful.  NullObjects of
some kind?

The GraphManager ought not to have defaults for SourceSkepticism - keep that in
the RemoteHost or other client code. Likewise, GM doesn't start GraphFoci. Both
of those are the result of scope creep on GM.

Remove the _:local context - it doesn't make sense on the GM. Writes to a GM
should come from 3 places:

 * A GraphFocus, that has an implicit context to write to.
 * Raw #insert by client code - in which case, vaya con Dios
 * #insert_document, which has an explicit context to write to.

GM's should also accept non-ResourceQuery|Patterns. Simple to RQ|P.from(q|p),
with infered contexts.

Infered contexts raises this interesting point: every pattern potentially has 6
contexts to consider as credible:

 * Its context
 * Its subject
 * Its object
 * Contexts mentioned in its query
 * Subjects mentioned in its query
 * Objects mentioned in its query

(Not every pattern will have all 6) (Also - more than one of any may result in
an empty result - a Query with two contexts I *think* means "statements must
have this context and that context" which is impossible.)

That said, these resources might then form the basis of a credence review


*** Pure Mad Science ***

Omniscient test server.

Basically: RemoteHost collects the requests that get made and records them.
Play against server, and then "flatten" resulting meta-graph. TestServer simply
replies to everything with flattened graph - possibly changing state (graph) on
PUT/POST/DELETE.

Two sets of test files:

client -> server requests, the responses of which can be tested and recorded

server -> client responses, used as fixtures for client tests - right things
displayed / correct POSTs made.


Single Graph Update
client should be able to alter the graph as needed, build
a single sub-graph that contains all their modifications, and PUT the graph to
a single resource. With parcelling, it's reasonable to break the graph up and
distribute to Resource models, but the challenge is at the If-Modified-Since
step: the graph has to be broken up, and we have to check the Modified time of
all concerned resources.  (Although existing Parcel.resources_for() works...)

There also doesn't seem to be a way to manage Etag If-Mod behavior...
</->
We'd have to collect error messages and decide which status takes precedence



Statistical Front Loading
based on human definitions of resource graphs, and
emperical collection of client behavior (e.g. GET resourceB, Referer:
resourceA, (X-)Triggering-Property: propC), determine properties to front-load
into requests for resourceA to reduce the likelihood of subsequent request for
B. Contributing factors include the size of the extra data, impact on cacheing,
actual impact on subsequent requests (since propC shadows some non-negative
number of other properties we need about resourceB.)

Consider case of an list of users page - client wants names and roles for all
users. So UserList -> User1,User2,User3 etc because of foaf:name and also
authn:role. Once we front load both foaf:name and authn:name, UserList ->
User17, because that's the particular user we want. But: foaf:name makes not
impact by itself (except to change the triggering property), and both aren't an
absolute impact. On the other extreme, front-loading everything from User into
UserList probably stops requests for User17, but the network transfer is
greater as a result.

Certainly, there's a tendancy for everything to be front loaded, and as clients
change we might need to experimentally roll back a front-loading to see if that
triggers more requests.
